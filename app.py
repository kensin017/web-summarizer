import streamlit as st
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import openai
import time
from openai import RateLimitError

st.set_page_config(page_title="GPT ì›¹í˜ì´ì§€ ìš”ì•½ê¸°", layout="wide")

# ğŸ”‘ API í‚¤ ì„¤ì • (Streamlit Secrets ì‚¬ìš©)
client = openai.OpenAI(api_key=st.secrets["OPENAI_API_KEY"])

st.title("ğŸ§  GPT ì›¹í˜ì´ì§€ ìš”ì•½ê¸°")
st.write("ì›¹í˜ì´ì§€ ë³¸ë¬¸ì„ ìë™ìœ¼ë¡œ ìš”ì•½í•´ë“œë¦½ë‹ˆë‹¤. í•˜ìœ„ ë§í¬ê¹Œì§€ í¬í•¨ë©ë‹ˆë‹¤.")

# âœ… ë§í¬ ìˆ˜ì§‘
def extract_links(url):
    base_domain = urlparse(url).netloc
    try:
        res = requests.get(url, timeout=5)
        soup = BeautifulSoup(res.text, 'html.parser')
        links = set()
        for tag in soup.find_all('a', href=True):
            href = urljoin(url, tag['href'])
            if base_domain in urlparse(href).netloc:
                links.add(href.split('#')[0])
        return list(links)
    except Exception as e:
        st.error(f"ë§í¬ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        return []

# âœ… í…ìŠ¤íŠ¸ ì¶”ì¶œ
def extract_text(url):
    try:
        res = requests.get(url, timeout=5)
        soup = BeautifulSoup(res.text, 'html.parser')
        for tag in soup(['script', 'style', 'header', 'footer', 'nav']):
            tag.decompose()
        return soup.get_text(separator=' ', strip=True)
    except Exception as e:
        st.warning(f"ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨: {url} â†’ {e}")
        return ""

# âœ… ìš”ì•½ (RateLimit ëŒ€ì‘ í¬í•¨)
def summarize_text(text):
    prompt = f"ë‹¤ìŒ ì›¹ í˜ì´ì§€ ë‚´ìš©ì„ í•µì‹¬ë§Œ ìš”ì•½í•´ì¤˜:\n{text}"
    for i in range(3):  # ìµœëŒ€ 3íšŒ ì¬ì‹œë„
        try:
            response = client.chat.completions.create(
                model="gpt-4o",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.5
            )
            return response.choices[0].message.content
        except RateLimitError:
            wait = 2 ** i
            st.warning(f"ìš”ì²­ì´ ë§ì•„ ëŒ€ê¸° ì¤‘ì…ë‹ˆë‹¤... ({wait}ì´ˆ)")
            time.sleep(wait)
        except Exception as e:
            return f"ìš”ì•½ ì‹¤íŒ¨: {e}"
    return "âŒ ìš”ì•½ ìš”ì²­ì´ ë„ˆë¬´ ë§ì•„ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."

# âœ… ì…ë ¥ì°½
url = st.text_input("ğŸ”— ì›¹í˜ì´ì§€ ì£¼ì†Œ ì…ë ¥", placeholder="https://example.com")

if st.button("ìš”ì•½ ì‹œì‘") and url:
    with st.spinner("ğŸ” ë§í¬ ìˆ˜ì§‘ ì¤‘..."):
        all_links = [url] + extract_links(url)
    
    with st.spinner("ğŸ“„ ë³¸ë¬¸ ìˆ˜ì§‘ ì¤‘..."):
        texts = [extract_text(link) for link in all_links]
        merged_text = "\n\n".join(texts)[:8000]  # ìµœëŒ€ í† í° ì œí•œ ëŒ€ì‘

    with st.spinner("ğŸ§  GPT ìš”ì•½ ì¤‘..."):
        summary = summarize_text(merged_text)

    st.subheader("ğŸ“‹ ìš”ì•½ ê²°ê³¼")
    st.write(summary)
    st.download_button("ğŸ“¥ ìš”ì•½ ê²°ê³¼ ë‹¤ìš´ë¡œë“œ", summary, file_name="summary.txt")
